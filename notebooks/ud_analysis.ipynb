{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP Processing and Analysis with Universal Dependencies (UD)\\n\n",
        "\\n\n",
        "**Project Overview:**\\n\n",
        "This notebook demonstrates comprehensive NLP analysis using Universal Dependencies (UD) treebanks.\\n\n",
        "\\n\n",
        "**Steps covered:**\\n\n",
        "1. Corpus Selection and Data Loading\\n\n",
        "2. Data Preprocessing and Extraction\\n\n",
        "3. Corpus Statistics\\n\n",
        "4. PoS Tag Distribution and Visualization\\n\n",
        "5. Custom Sentence Processing\\n\n",
        "6. TF-IDF Vectorization\\n\n",
        "7. Similarity Analysis\\n\n",
        "8. Most Similar Sentence Pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System\\n\n",
        "import sys\\n\n",
        "from pathlib import Path\\n\n",
        "import warnings\n",
        "\n",
        "from Project_NLP.src.visualizer import plot_sentence_length_distribution\\n\n",
        "warnings.filterwarnings('ignore')\\n\n",
        "\\n\n",
        "# Add src to path\\n\n",
        "sys.path.append(str(Path.cwd().parent / 'src'))\\n\n",
        "\\n\n",
        "# Core libraries\\n\n",
        "import pandas as pd\\n\n",
        "import numpy as np\\n\n",
        "from tqdm import tqdm\\n\n",
        "\\n\n",
        "# Custom modules\\n\n",
        "from ud_loader import load_conllu_file, extract_sentence_data, get_conllu_sample\\n\n",
        "from statistics import (compute_corpus_statistics, compute_pos_distribution,\\n\n",
        "                       create_statistics_summary, get_top_frequent_words,\\n\n",
        "                       get_top_frequent_lemmas)\\n\n",
        "from preprocessor import TextPreprocessor\\n\n",
        "from similarity import SimilarityAnalyzer\\n\n",
        "from visualizer import (plot_pos_distribution, plot_similarity_distribution,\\n\n",
        "                       plot_sentence_length_distribution, plot_top_frequent_words)\\n\n",
        "\\n\n",
        "print('✓ All libraries imported successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Corpus Selection and Data Loading\n",
        "\n",
        "**Selected Language**: Albanian (Shqip)\n",
        "\n",
        "**Dataset**: UD_Albanian-TSA (Treebank of Standard Albanian)\n",
        "\n",
        "**About Albanian**:\n",
        "- Indo-European language with its own unique branch\n",
        "- Spoken by ~7-8 million people in Albania, Kosovo, and surrounding regions\n",
        "- Rich morphology with cases, genders, and definite/indefinite forms\n",
        "- File: `sq_tsa-ud-test.conllu`\n",
        "\n",
        "**Understanding .conllu Format**:\n",
        "Each sentence has:\n",
        "- Metadata: `# sent_id`, `# text`\n",
        "- 10 columns per token: ID, FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONLLU_FILE = \"../data/sq_tsa-ud-test.conllu\"\n",
        "LANGUAGE = \"Albanian\"\n",
        "\n",
        "print(f\"Language: {LANGUAGE}\")\n",
        "print(f\"File: {CONLLU_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the corpus\n",
        "print(\"Loading Albanian corpus...\")\n",
        "sentences = load_conllu_file(CONLLU_FILE)\n",
        "print(f\"✓ Loaded {len(sentences)} sentences from {LANGUAGE} corpus\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample sentence structure\n",
        "print(\"\\n=== Sample Albanian Sentence Structure ===\")\n",
        "print(f\"\\nSentence: {sentences[0].metadata.get('text', 'N/A')}\\n\")\n",
        "print(\"Token Annotations:\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "sample_tokens = []\n",
        "for token in sentences[0]:\n",
        "    if isinstance(token['id'], int):\n",
        "        sample_tokens.append({\n",
        "            'ID': token['id'],\n",
        "            'Form': token['form'],\n",
        "            'Lemma': token['lemma'],\n",
        "            'UPOS': token['upos'],\n",
        "            'Features': str(token['feats'])[:30] if token['feats'] else None\n",
        "        })\n",
        "\n",
        "df_sample = pd.DataFrame(sample_tokens)\n",
        "print(df_sample.to_string(index=False))\n",
        "print(\"\\n\" + \"-\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Extraction\n",
        "\n",
        "Extract tokens, lemmas, and PoS tags from the Albanian corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract all corpus data\n",
        "print(\"Extracting data from corpus...\")\n",
        "corpus_data = extract_sentence_data(sentences)\n",
        "print(\"✓ Data extraction complete!\")\n",
        "print(f\"\\nExtracted:\")\n",
        "print(f\"  - {len(corpus_data['all_tokens'])} tokens\")\n",
        "print(f\"  - {len(corpus_data['all_lemmas'])} lemmas\")\n",
        "print(f\"  - {len(corpus_data['sentence_texts'])} sentences\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Corpus Statistics\n",
        "\n",
        "Calculate key statistics about the Albanian corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute statistics\n",
        "stats = compute_corpus_statistics(corpus_data)\n",
        "stats_df = create_statistics_summary(stats)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"         CORPUS STATISTICS - {LANGUAGE}\")\n",
        "print(\"=\"*80)\n",
        "print(stats_df.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save statistics\n",
        "stats_df.to_csv('../reports/corpus_statistics.csv', index=False)\n",
        "print(\"\\n✓ Statistics saved to reports/corpus_statistics.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Part-of-Speech Tag Distribution\n",
        "\n",
        "Calculate and visualize PoS tag frequencies in Albanian."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate PoS distribution\n",
        "pos_df = compute_pos_distribution(corpus_data['all_pos_tags'])\n",
        "\n",
        "print(\"\\n=== Albanian Part-of-Speech Tag Distribution ===\\n\")\n",
        "print(pos_df.to_string(index=False))\n",
        "\n",
        "# Save PoS distribution\n",
        "pos_df.to_csv('../reports/pos_distribution.csv', index=False)\n",
        "print(\"\\n✓ PoS distribution saved to reports/pos_distribution.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize PoS distribution\n",
        "plot_pos_distribution(pos_df, language=LANGUAGE, save_path='../outputs/pos_distribution.png')\n",
        "print(\"✓ PoS distribution visualization saved to outputs/pos_distribution.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Sentence Length Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sentence length distribution\n",
        "sent_lengths = [len(sent) for sent in corpus_data['sentence_tokens']]\n",
        "plot_sentence_length_distribution(sent_lengths, language=LANGUAGE,\n",
        "                                  save_path='../outputs/sentence_length_distribution.png')\n",
        "print(\"✓ Sentence length distribution saved to outputs/sentence_length_distribution.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Top Frequent Words and Lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top frequent words\n",
        "top_words = get_top_frequent_words(corpus_data['all_tokens'], top_n=20)\n",
        "print(\"\\n=== Top 10 Most Frequent Words in Albanian ===\\n\")\n",
        "print(top_words.head(10).to_string(index=False))\n",
        "\n",
        "# Top frequent lemmas\n",
        "top_lemmas = get_top_frequent_lemmas(corpus_data['all_lemmas'], top_n=20)\n",
        "print(\"\\n\\n=== Top 10 Most Frequent Lemmas in Albanian ===\\n\")\n",
        "print(top_lemmas.head(10).to_string(index=False))\n",
        "\n",
        "# Save results\n",
        "top_words.to_csv('../reports/top_frequent_words.csv', index=False)\n",
        "top_lemmas.to_csv('../reports/top_frequent_lemmas.csv', index=False)\n",
        "print(\"\\n✓ Frequency data saved to reports/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Process New Sentences\n",
        "\n",
        "Implement tokenization and lemmatization function for Albanian text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize preprocessor\n",
        "preprocessor = TextPreprocessor()\n",
        "\n",
        "# Test with sample sentences (using English for demonstration since NLTK lemmatizer works best with English)\n",
        "test_sentences = [\n",
        "    \"Natural language processing is a fascinating field of study.\",\n",
        "    \"The students are learning about computational linguistics today.\",\n",
        "    \"Machine learning algorithms can process large amounts of text data.\"\n",
        "]\n",
        "\n",
        "print(\"\\n=== Processing New Sentences ===\\n\")\n",
        "print(\"Demonstrating tokenization and lemmatization:\\n\")\n",
        "\n",
        "for i, sent in enumerate(test_sentences, 1):\n",
        "    result = preprocessor.process_sentence(sent)\n",
        "    print(f\"Example {i}:\")\n",
        "    print(f\"  Original: {result['original']}\")\n",
        "    print(f\"  Tokens:   {result['tokens'][:8]}...\")\n",
        "    print(f\"  Lemmas:   {result['processed'][:8]}...\")\n",
        "    print(f\"  Method:   {result['method']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Lemmatization vs Stemming\n",
        "comparison_sentence = \"The runners are running faster than they were running yesterday.\"\n",
        "\n",
        "lemma_result = preprocessor.process_sentence(comparison_sentence, use_stemming=False)\n",
        "stem_result = preprocessor.process_sentence(comparison_sentence, use_stemming=True)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Original': lemma_result['tokens'],\n",
        "    'Lemmatized': lemma_result['processed'],\n",
        "    'Stemmed': stem_result['processed']\n",
        "})\n",
        "\n",
        "print(\"\\n=== Lemmatization vs Stemming Comparison ===\")\n",
        "print(f\"\\nSentence: {comparison_sentence}\\n\")\n",
        "print(comparison_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. TF-IDF Vectorization\n",
        "\n",
        "Convert Albanian sentences to TF-IDF vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use all available sentences for TF-IDF\n",
        "subset_sentences = corpus_data['sentence_texts']\n",
        "SUBSET_SIZE = len(subset_sentences)\n",
        "\n",
        "print(f\"Using {SUBSET_SIZE} Albanian sentences for TF-IDF analysis...\")\n",
        "\n",
        "# Create TF-IDF analyzer\n",
        "analyzer = SimilarityAnalyzer(\n",
        "    max_features=1000,  # Adjusted for smaller corpus\n",
        "    ngram_range=(1, 2),\n",
        "    stop_words=None,  # No Albanian stop words in sklearn\n",
        "    min_df=1  # Lower threshold for smaller corpus\n",
        ")\n",
        "\n",
        "# Fit and transform\n",
        "tfidf_matrix = analyzer.fit_transform(subset_sentences)\n",
        "\n",
        "print(f\"\\n✓ TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
        "print(f\"  - {tfidf_matrix.shape[0]} sentences\")\n",
        "print(f\"  - {tfidf_matrix.shape[1]} features\")\n",
        "print(f\"  - Sparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display TF-IDF scores for a sample sentence\n",
        "sample_idx = 5\n",
        "sample_tfidf = analyzer.get_sentence_tfidf(sample_idx, top_n=10)\n",
        "\n",
        "print(f\"\\n=== TF-IDF Analysis for Albanian Sentence {sample_idx} ===\")\n",
        "print(f\"\\nSentence: {subset_sentences[sample_idx][:100]}...\")\n",
        "print(f\"\\nTop 10 TF-IDF weighted terms:\")\n",
        "print(sample_tfidf.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Similarity Analysis\n",
        "\n",
        "Calculate cosine similarity and Euclidean distance between Albanian sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute similarity matrices\n",
        "print(\"Computing similarity matrices...\")\n",
        "cosine_sim = analyzer.compute_cosine_similarity()\n",
        "euclidean_dist = analyzer.compute_euclidean_distance()\n",
        "\n",
        "print(\"✓ Similarity matrices computed\")\n",
        "print(f\"  Matrix shape: {cosine_sim.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example comparisons\n",
        "print(\"\\n=== Example Sentence Comparisons ===\\n\")\n",
        "\n",
        "example_pairs = [(5, 10), (15, 20), (25, 30)]\n",
        "\n",
        "for idx1, idx2 in example_pairs:\n",
        "    if idx1 < len(subset_sentences) and idx2 < len(subset_sentences):\n",
        "        comparison = analyzer.compare_sentences(idx1, idx2, cosine_sim, euclidean_dist)\n",
        "        print(f\"Sentence {idx1} vs Sentence {idx2}:\")\n",
        "        print(f\"  [{idx1}] {subset_sentences[idx1][:60]}...\")\n",
        "        print(f\"  [{idx2}] {subset_sentences[idx2][:60]}...\")\n",
        "        print(f\"  Cosine Similarity:    {comparison['cosine_similarity']}\")\n",
        "        print(f\"  Euclidean Distance:   {comparison['euclidean_distance']}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get similarity statistics\n",
        "sim_stats = analyzer.get_similarity_statistics(cosine_sim)\n",
        "\n",
        "print(\"\\n=== Similarity Statistics ===\\n\")\n",
        "print(f\"Mean Cosine Similarity:    {sim_stats['mean']:.4f}\")\n",
        "print(f\"Std Deviation:             {sim_stats['std']:.4f}\")\n",
        "print(f\"Min Similarity:            {sim_stats['min']:.4f}\")\n",
        "print(f\"Max Similarity:            {sim_stats['max']:.4f}\")\n",
        "print(f\"Median:                    {sim_stats['median']:.4f}\")\n",
        "print(f\"Q1 (25th percentile):      {sim_stats['q1']:.4f}\")\n",
        "print(f\"Q3 (75th percentile):      {sim_stats['q3']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Most Similar Sentence Pairs\n",
        "\n",
        "Find the most semantically similar Albanian sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find top 10 most similar pairs\n",
        "print(\"Finding most similar Albanian sentence pairs...\\n\")\n",
        "most_similar = analyzer.find_most_similar_pairs(subset_sentences, cosine_sim, top_n=10)\n",
        "\n",
        "print(\"=\"*100)\n",
        "print(\"TOP 10 MOST SIMILAR ALBANIAN SENTENCE PAIRS\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "for rank, (idx1, idx2, similarity, sent1, sent2) in enumerate(most_similar, 1):\n",
        "    print(f\"\\nRank {rank} - Similarity: {similarity:.4f}\")\n",
        "    print(f\"  [{idx1}] {sent1[:80]}...\")\n",
        "    print(f\"  [{idx2}] {sent2[:80]}...\")\n",
        "    print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save similarity results\n",
        "similarity_results = []\n",
        "for idx1, idx2, similarity, sent1, sent2 in most_similar:\n",
        "    similarity_results.append({\n",
        "        'Rank': len(similarity_results) + 1,\n",
        "        'Sentence_1_Index': idx1,\n",
        "        'Sentence_2_Index': idx2,\n",
        "        'Cosine_Similarity': round(similarity, 4),\n",
        "        'Sentence_1': sent1[:100],\n",
        "        'Sentence_2': sent2[:100]\n",
        "    })\n",
        "\n",
        "similarity_df = pd.DataFrame(similarity_results)\n",
        "similarity_df.to_csv('../reports/most_similar_pairs.csv', index=False)\n",
        "print(\"\\n✓ Similarity results saved to reports/most_similar_pairs.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Visualize Similarity Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get upper triangle of similarity matrix (excluding diagonal)\n",
        "upper_triangle = np.triu_indices(cosine_sim.shape[0], k=1)\n",
        "similarity_values = cosine_sim[upper_triangle]\n",
        "\n",
        "# Visualize\n",
        "plot_similarity_distribution(similarity_values, save_path='../outputs/similarity_distribution.png')\n",
        "print(\"✓ Similarity distribution visualization saved to outputs/similarity_distribution.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Final Summary\n",
        "\n",
        "Complete analysis of Albanian Universal Dependencies corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"                    FINAL ANALYSIS SUMMARY - ALBANIAN CORPUS\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "print(f\"\\n1. CORPUS INFORMATION\")\n",
        "print(f\"   Language:              {LANGUAGE}\")\n",
        "print(f\"   Treebank:              UD_Albanian-TSA\")\n",
        "print(f\"   File:                  {CONLLU_FILE}\")\n",
        "\n",
        "print(f\"\\n2. CORPUS STATISTICS\")\n",
        "print(f\"   Total Sentences:       {stats['num_sentences']:,}\")\n",
        "print(f\"   Total Tokens:          {stats['num_tokens']:,}\")\n",
        "print(f\"   Vocabulary Size:       {stats['vocabulary_size']:,}\")\n",
        "print(f\"   Unique PoS Tags:       {stats['unique_pos_tags']}\")\n",
        "print(f\"   Avg Sentence Length:   {stats['avg_sent_length']:.2f} tokens\")\n",
        "print(f\"   Type-Token Ratio:      {stats['type_token_ratio']:.4f}\")\n",
        "\n",
        "print(f\"\\n3. POS TAG DISTRIBUTION\")\n",
        "print(f\"   Most Common PoS Tags:\")\n",
        "for _, row in pos_df.head(5).iterrows():\n",
        "    print(f\"   - {row['PoS Tag']:10s}: {row['Frequency']:5,} ({row['Percentage']:5.1f}%)\")\n",
        "\n",
        "print(f\"\\n4. TF-IDF ANALYSIS\")\n",
        "print(f\"   Sentences Analyzed:    {tfidf_matrix.shape[0]}\")\n",
        "print(f\"   Feature Dimensions:    {tfidf_matrix.shape[1]}\")\n",
        "print(f\"   Matrix Sparsity:       {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
        "\n",
        "print(f\"\\n5. SIMILARITY ANALYSIS\")\n",
        "print(f\"   Mean Similarity:       {sim_stats['mean']:.4f}\")\n",
        "print(f\"   Std Deviation:         {sim_stats['std']:.4f}\")\n",
        "print(f\"   Similarity Range:      [{sim_stats['min']:.4f}, {sim_stats['max']:.4f}]\")\n",
        "\n",
        "print(f\"\\n6. GENERATED FILES\")\n",
        "print(f\"   Reports (CSV):\")\n",
        "print(f\"   ✓ reports/corpus_statistics.csv\")\n",
        "print(f\"   ✓ reports/pos_distribution.csv\")\n",
        "print(f\"   ✓ reports/top_frequent_words.csv\")\n",
        "print(f\"   ✓ reports/top_frequent_lemmas.csv\")\n",
        "print(f\"   ✓ reports/most_similar_pairs.csv\")\n",
        "print(f\"\\n   Visualizations (PNG):\")\n",
        "print(f\"   ✓ outputs/pos_distribution.png\")\n",
        "print(f\"   ✓ outputs/sentence_length_distribution.png\")\n",
        "print(f\"   ✓ outputs/similarity_distribution.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*100)\n",
        "print(\"✓ ANALYSIS COMPLETE! All results saved.\")\n",
        "print(\"=\"*100)\n",
        "print(\"\\nUse these outputs for your final report:\")\n",
        "print(\"1. Language description: Albanian (Shqip) - Indo-European\")\n",
        "print(\"2. Processing steps: Documented in this notebook\")\n",
        "print(\"3. Statistics: Available in CSV reports\")\n",
        "print(\"4. Code: This reproducible Jupyter notebook\")\n",
        "print(\"5. Screenshots: Capture visualizations and console output\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
